


from sklearn.svm import SVC
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import datasets


df = datasets.load_iris(as_frame=True).frame
df.head()


df.isnull().sum()


df.shape


X = df.drop(columns = "target")
y = df["target"]



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify = y)



from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scale = scaler.fit_transform(X_train) 
X_test_scale = scaler.transform(X_test)


# model creation
svc = SVC()
svc.fit(X_train_scale, y_train)


y_pred = svc.predict(X_test_scale)

from sklearn.metrics import accuracy_score, classification_report

print("accuracy:", accuracy_score(y_test, y_pred))
print("classification_report: \n", classification_report(y_test, y_pred))


#linear kernel
svc = SVC(kernel="linear")
svc.fit(X_train_scale, y_train)

y_pred = svc.predict(X_test_scale)

print("accuracy:", accuracy_score(y_test, y_pred))


#polynomial kernel
svc = SVC(kernel="poly")
svc.fit(X_train_scale, y_train)

y_pred = svc.predict(X_test_scale)

print("accuracy:", accuracy_score(y_test, y_pred))


#sigmoid kernel
svc = SVC(kernel="sigmoid")
svc.fit(X_train_scale, y_train)

y_pred = svc.predict(X_test_scale)

print("accuracy:", accuracy_score(y_test, y_pred))


# changing C values ie slack penalty

C_vals = [0.5, 1, 2, 3, 4, 5]
for c_val in C_vals:
    svc = SVC(kernel = "rbf", C = c_val)
    svc.fit(X_train_scale, y_train)

    y_pred = svc.predict(X_test_scale)
    
    print("For C = ", c_val, "accuracy:", accuracy_score(y_test, y_pred))
    





from sklearn.svm import SVR
from sklearn.metrics import r2_score


df = datasets.load_diabetes(as_frame=True).frame
df.head()


df.shape


X = df.drop(columns = "target")
y = df["target"]



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)



scale = StandardScaler()
# here we are scaling only y not X because y is not scaled, and we are transforming it to 2D arr, and converting back it to 1d usinf ravel.
y_train_scale = scale.fit_transform(y_train.values.reshape(-1,1)).ravel() 
y_test_scale = scale.transform(y_test.values.reshape(-1,1)).ravel()


svr = SVR()
svr.fit(X_train, y_train_scale)


# for this model we are checking train and testing results
y_test_pred = svr.predict(X_test)
y_train_pred = svr.predict(X_train)

print("r2_score_train:", r2_score(y_train_scale, y_train_pred))
print("r2_score_test:", r2_score(y_test_scale, y_test_pred))
# And as you can see our model is overfitting training is good testing is bad


# linear kernel
svr = SVR(kernel="linear")
svr.fit(X_train, y_train_scale)

y_pred = svr.predict(X_test)

print("r2_score:", r2_score(y_test_scale, y_pred))


# polynomial kernel
svr = SVR(kernel="poly")
svr.fit(X_train, y_train_scale)

y_pred = svr.predict(X_test)

print("r2_score:", r2_score(y_test_scale, y_pred))


# sigmoid kernel
svr = SVR(kernel="sigmoid")
svr.fit(X_train, y_train_scale)

y_pred = svr.predict(X_test)

print("r2_score:", r2_score(y_test_scale, y_pred))





from sklearn.model_selection import GridSearchCV


param_grid = {
    "C": [1, 2, 5, 10, 50, 100],
    "kernel": ["rbf", "linear"],
    "epsilon": [0.01, 0.1, 0.2, 0.3, 0.5]
}


print("best params:", grid_search.best_params_)


best_model = SVR(kernel="linear", epsilon=0.1, C=5)
best_model.fit(X_train, y_train_scale)


y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

print("r2_score_train:", r2_score(y_train_scale, y_train_pred))
print("r2_score_test:", r2_score(y_test_scale, y_test_pred))



