import requests
# The requests library is a Python tool used to send HTTP requests to websites or APIs to collect data.


URL = "https://stephen-king-api.onrender.com/api/books"
res = requests.get(URL)


json_data = res.json()


import pandas as pd
pd.json_normalize(json_data["data"])


URL = "https://api.imgflip.com/get_memes"
res = requests.get(URL)


json_data = res.json()
json_data


pd.json_normalize(json_data)


# Web Scraping - Web scraping is used to gather publicly available data from websites for analysis or storage.

import requests
URL = "https://www.scrapethissite.com/pages/simple/"

res = requests.get(URL)



print(res.status_code)


if res.status_code == 200:
    print(res.text)
    print(res.content)
    print(res.headers)


# Here we have created a folder and file using file handling conncepts encoding is nothing but in windows we need to specify this then simply wrote the text from the page into the file
with open("scrap_data_folder/data1.html", "w", encoding="utf-8") as f:
    f.write(res.text)


# Beautifulsoup - BeautifulSoup (bs4) is a Python library used to parse HTML and XML documents. It helps you extract data from webpages after downloading them with requests.

from bs4 import BeautifulSoup

with open("scrap_data_folder/data1.html", "r") as f:
    html_content = f.read()
soup = BeautifulSoup(html_content, "lxml")


soup


#Methods in BeautifulSoup

soup.find()
soup.find_all()
soup.find_parent()
soup.find_next()
soup.find_previous()
#CSS selector
select()
select_one()
#Attributes
get()
.attrs()
#DOM
parent
children
descendants
next_sibling()
previous_sibling()



# Assignment 1
#Cleaning and extracting the scrap data 

all_h3 = soup.find_all("h3")

all_countries = []

for h3 in all_h3:
    name = h3.get_text(strip=True)
    # print(h3.find_parent("div")["class"])

    population = h3.find_next("div").select_one("span.country-population").get_text(strip=True)
    print(h3.find_next("div").select_one("span.country-population").get_text(strip=True))

    all_countries.append((name, population))


# Converted the data into a dataframe
df = pd.DataFrame(all_countries, columns=["Name", "Population"])


#Created a new folder and a file in it and stored the data there(to_csv bcz we are writing the data into the csv file)
df.to_csv("clean_data_folder/data1.csv", index=False)


# Assignment 2
#Also here we will download the data in a automated manner
#check GPT if not able to recall

page_count = 1

while True:
    URL = f"https://quotes.toscrape.com/page/{page_count}/"
    res = requests.get(URL)

    soup = BeautifulSoup(res.text, "lxml")
    quotes = soup.select("div.quote")

    if not quotes:
        print("no valid pages anymore...")
        break

    with open(f"scrap_data_folder/page_{page_count}.html", "w", encoding="utf-8") as f:
        f.write(res.text)
        print(f"downloaded data from page{page_count}")

    page_count = page_count + 1


#Cleaning and extracting the scrap data

with open("scrap_data_folder/page_1.html", "r", encoding="utf-8") as f:
    html_content = f.read()
soup = BeautifulSoup(html_content, "lxml")


soup



