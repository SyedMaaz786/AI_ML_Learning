import pandas as pd


info = {
    "Name": ["Syed", "Maaz", "Khan"],
    "Age": [20, 22, 23]
}
df = pd.DataFrame(info)
print(df)


#Index-Based access
s = pd.Series([1,2,3,4,5])
print(s)
print(type(s))
print(s[0])
print(s[3])


#Label-Based access
s = pd.Series([20, 22, 23, 24], index = ["Syed", "Maaz", "Khan", "Mohd"])
print(s)
print(s["Maaz"])
print(s["Mohd"])
print(s.index)


#vectorization
s1 = pd.Series([1,2,3,4,5])
s2 = pd.Series([10,20,30,40,50])
print(s1 + s2)

# Also remember series are mutable but the size of the series is immutable.


#Dataframe

info = {
    "Name": ["Syed", "Maaz", "Khan"],
    "Age": [20, 22, 24],
    "CGPA": [7.5, 8.5, 9.5]
}
df = pd.DataFrame(info)
print(df, type(df))
df
print(df.index)
print(df.columns)


#Dataframe
df = pd.DataFrame(
    [
        ["Syed", 20],
        ["Maaz", 22],
        ["Khan", 24]
    ], columns = ["Name", "Age"] #Here we are explicitly naming the columns
)
print(df)


#Dataframe in numpy
import numpy as np

np_arr = np.array(
    [
        [1,2,3,4],
        [5,6,7,8]
    ]
)
df = pd.DataFrame(np_arr, columns = ["A", "B", "C", "D"])
print(df)


#Reading data from csv file
df = pd.read_csv("pandas_data.csv")
print(df, type(df))


#Reading data form json file
df = pd.read_json("pandas_data.json")
print(df, type(df))


#Methods in printing the file data
df.head() #prints starting 5 rows
df.head(4) #prints as customised
df.tail() #prints ending 5 rows
df.tail(3) #prints as customised
df.sample() #prints random row
df.sample(5) #prints as customised
df.info() #prints the info of file
df.shape  #prints the shape(rows and colums)
df.describe #gives the value of numerical data from the file
df.columns
df.nunique()


#assignment 1 
df = pd.read_csv("pandas_assignment_data.csv")
df


#selecting or accessing data

#column data
df["city"]
df [
    ["city",
    "temperature"]
]

#row data
df.loc[0]  
df.loc[0:2] #start idx : end idx (inclusive) GPT
df.iloc[2]
df.iloc[0:2] #start idx : end idx (exclusive) GPT

#individual cells

df.loc[0:2, "city"]  #remember in loc we can directly pass the column name which we want to print
df.columns

df.iloc[0:3, 2]      #but in iloc we need to pass the column index we cant pass the value iloc won't take string as the value

df.at[0, "city"]     # same as loc and iloc but here we can select only one element
df.iat[0, 2]

#Remember all this data being displayed is only a view not a copy.
df= pd.read_csv("pandas_assignment_data.csv").copy() #Better practice everytime before starting with your work make a copy so that the og data can be changed.
print(df)


#Filtering of data

#On basis of operators
df[ df["aqi"] > 100 ]
df[ df["aqi"] > 100 & (df["temperature"] > 30) ]
#many more check GPT or try with another methods.

#Filtering of data(Query methods)
df.query("aqi > 100")  #remember you can use variable inside the query using @ and the name of the variable
df.query("aqi > 100 & temperature > 30")


#Assignment 2 

Data cleaning part1
df = pd.read_csv("pandas_assignment2_data.csv")

cleaned_data = df.copy()
df.isnull() #returns true if the value is missing
df.isnull().sum() #gives the sum of all the missing values
df.dropna()  #drops the missing rows
df.dropna(axis = 1) # drops the missing col
df.fillna(0) #fills the missing values with 0 or (pass any other values which you want to fill)
cleaned_data["age"] = cleaned_data["age"].fillna(20) #only in the specific label
df.ffill() #fills the empty element with the same element as before(runs forward)
df.bfill() #fills the empty element with the same element as before(runs backward)

cleaned_data


#Data cleaning part2

df.duplicated() #Displays the duplicates values in the bool format
df["country"]duplicated() #by using the columns(you can also do it by rows or anyother filteration as well)
df.drop_duplicates() #drops duplicates 


# Data types

#Here we have done type conversion where age was float we changed it to int
df.dtypes
df2 = df.copy()

df2 = df2.fillna(1)
df2["age"] = df2["age"].astype("int64").copy()
df2.dtypes

#conversion to datetime
pd.to_datetime() #This will convert the type to datetime GPT for more info

#string

df["gender"].str.upper()
df["gender"].str.lower()
df["gender"].str.capitalize()
df["name"].str.split(" ")
df["country"].str.contains("india")
#Many more refer doc or GPT


#Data transformation (feature engineering)

#apply()
#Here we have added a new col of tax and assigned the values usong lambda
df2 = df.copy()  #remember here x is income we have assigned on lambda
df2["tax"] = df2["income"].apply(lambda x: "20%" if x >= 50000 else "10%")
df2


#map()

#(Here we are changing values from Male=M, female=F, unknown=U)
gender_map = {"Male": "M", "Female": "F", "Unknown": "U"}
df2["gender"].map(gender_map)



#assign()

#(Here we are assigning the new income col hiking 10% increase in the salary)

df2.assign(new_income = df2["income"] * 1.1)




#replace()

#(Here we are replacing the old value with the new value)

df2["country"].replace("USA", "United States")


#rename()

#Here we are renaming the col

df2.rename(columns = {"income": "Salary"})
df2.rename(index = {1 : "First"})


#Sort()

df2.sort_values("income")  #ascending
df2.sort_values("income", ascending = False) #decending

df2.sort_index()

df2.reset_index() #If you want you can drop the old index by using drop command


#Ranking()

df2["rank"] = df2["id"].rank() #Here ranking is done on the basis of id highest id higher rank
df2

#reorder() - reorders based on the value being passed chech GPT



#Groupby and aggregate

#Here mean min and max are the aggregate fnx
df.groupby("country")["income"].mean()
df.groupby("country")["income"].min()
df.groupby("country")["income"].max()

#All at once
df.groupby("country")["income"].agg(["mean","min","max"])


# Melt and Pivot

df = pd.DataFrame({
    "country": ["USA", "USA", "India", "India"],
    "year": [2020, 2021, 2020, 2021],
    "sales": [100, 120, 90, 110],
    "profit": [20, 25, 18, 22]
})

# Melt - Wide to long
# Here we have melted the data these 4 var names should be same always where (id_vars = the values we want to preserve), (value_vars = the values we want to melt), (var_name = new col name), (value_name = new col values, so for this case sales and profit values will be dumped here)
melted_df = df.melt(
    id_vars=["country","year"],
    value_vars=["sales","profit"],
    var_name="metrics",
    value_name="value"
)

# Pivot - long to wide
# Here we have restored the data from melted to original form it takes 3 vars which should be same (index = this should be unique)
original = melted_df.pivot(
    index=["country", "year"],
    columns="metrics",
    values="value"
)



melted_df


original


# Merging and Joining Data

df_customers = pd.DataFrame({
    "customer_id": [1, 2, 3, 4],
    "name": ["Adam", "Bob", "Charlie", "Dave"]
})

df_orders = pd.DataFrame({
    "order_id": [101, 102, 103, 104],
    "customer_id": [2, 1, 4, 5],
    "amount": [250, 120, 300, 180]
})


df_customers


df_orders


# Merge - Merging 2 tables and, "on" tells pandas which common column to use (remember the column we are assigning in "on" should be present in both the tables)

pd.merge(df_customers, df_orders, on="customer_id") #inner join




pd.merge(df_customers, df_orders, on="customer_id", how="left") #left join - left table values will be same and right side values will be changed to NAN if the value is empty.


pd.merge(df_customers, df_orders, on="customer_id", how="right") #right join - right side table will be same and left side table values will be changed to NAN if the data is missing(remember in this case left table is customers and right table is orders)


pd.merge(df_customers, df_orders, on="customer_id", how="outer") #outer join - This will merge both the tables and replace NAN where ever the value is empty



