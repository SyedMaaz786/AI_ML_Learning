{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc5f75f-e5ce-4df5-a8ec-3b9afac51485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# The requests library is a Python tool used to send HTTP requests to websites or APIs to collect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21258eb0-d236-45e8-9544-7002e23af0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data collection\n",
    "URL = \"https://stephen-king-api.onrender.com/api/books\"\n",
    "res = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a780de83-d4f6-4022-a6d5-7b0085301062",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = res.json()\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13525cdc-51f6-4d70-85d6-774fc6ffd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.json_normalize(json_data[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe4c6f8e-e6e2-4d07-a641-3682ac0051e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homework problem\n",
    "URL = \"https://api.imgflip.com/get_memes\"\n",
    "res = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "880a23ae-31e7-438d-8c29-f423fc011338",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = res.json()\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1034b8e3-c24c-4938-b02d-07b4e86efe91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "      <th>data.memes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>[{'id': '181913649', 'name': 'Drake Hotline Bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   success                                         data.memes\n",
       "0     True  [{'id': '181913649', 'name': 'Drake Hotline Bl..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11326f33-3b25-434e-bbeb-eabdbb79e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping - Web scraping is used to gather publicly available data from websites for analysis or storage.\n",
    "\n",
    "import requests\n",
    "URL = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "\n",
    "res = requests.get(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e171714-fba2-41e8-9052-fcbc10c3e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(res.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4cbb37bc-6bd4-41f9-be23-5d133239bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if res.status_code == 200:\n",
    "    print(res.text)\n",
    "    print(res.content)\n",
    "    print(res.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bf79f4c-c371-4d65-99af-66902da87a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have created a folder and file using file handling conncepts encoding is nothing but in windows we need to specify this then simply wrote the text from the page into the file\n",
    "with open(\"scrap_data_folder/data1.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8350b52b-6ce5-4530-b112-8dd699255f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautifulsoup - BeautifulSoup (bs4) is a Python library used to parse HTML and XML documents. It helps you extract data from webpages after downloading them with requests.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"scrap_data_folder/data1.html\", \"r\") as f:\n",
    "    html_content = f.read()\n",
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d51b4-e103-4ac9-b2aa-61ef60c3288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7139099d-ad22-44b5-adca-b72a8b2595d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods in BeautifulSoup\n",
    "\n",
    "soup.find()\n",
    "soup.find_all()\n",
    "soup.find_parent()\n",
    "soup.find_next()\n",
    "soup.find_previous()\n",
    "#CSS selector\n",
    "select()\n",
    "select_one()\n",
    "#Attributes\n",
    "get()\n",
    ".attrs()\n",
    "#DOM\n",
    "parent\n",
    "children\n",
    "descendants\n",
    "next_sibling()\n",
    "previous_sibling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0e9b-77d5-4392-98a0-5555e0d9dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "#Cleaning and extracting the scrap data \n",
    "\n",
    "all_h3 = soup.find_all(\"h3\")\n",
    "\n",
    "all_countries = []\n",
    "\n",
    "for h3 in all_h3:\n",
    "    name = h3.get_text(strip=True)\n",
    "    # print(h3.find_parent(\"div\")[\"class\"])\n",
    "\n",
    "    population = h3.find_next(\"div\").select_one(\"span.country-population\").get_text(strip=True)\n",
    "    print(h3.find_next(\"div\").select_one(\"span.country-population\").get_text(strip=True))\n",
    "\n",
    "    all_countries.append((name, population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d18e592a-7bfb-42b4-8bdd-78bad4d67785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted the data into a dataframe\n",
    "df = pd.DataFrame(all_countries, columns=[\"Name\", \"Population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a720279-6a10-4404-810c-6a821a881e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a new folder and a file in it and stored the data there(to_csv bcz we are writing the data into the csv file)\n",
    "df.to_csv(\"clean_data_folder/data1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20c140c-28dd-45d9-a1b2-1909cdcd35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded data from page1\n",
      "downloaded data from page2\n",
      "downloaded data from page3\n",
      "downloaded data from page4\n",
      "downloaded data from page5\n",
      "downloaded data from page6\n",
      "downloaded data from page7\n",
      "downloaded data from page8\n",
      "downloaded data from page9\n",
      "downloaded data from page10\n",
      "no valid pages anymore...\n"
     ]
    }
   ],
   "source": [
    "# Assignment 2\n",
    "#here we will download the data in a automated manner\n",
    "#check GPT if not able to recall\n",
    "\n",
    "page_count = 1\n",
    "\n",
    "while True:\n",
    "    URL = f\"https://quotes.toscrape.com/page/{page_count}/\"\n",
    "    res = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    quotes = soup.select(\"div.quote\")\n",
    "\n",
    "    if not quotes:\n",
    "        print(\"no valid pages anymore...\")\n",
    "        break\n",
    "\n",
    "    with open(f\"scrap_data_folder/page_{page_count}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(res.text)\n",
    "        print(f\"downloaded data from page{page_count}\")\n",
    "\n",
    "    page_count = page_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1bc999-4d30-4cbd-abfa-dceee1c61b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and extracting the scrap data\n",
    "\n",
    "with open(\"scrap_data_folder/page_1.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc86aa9-9ef9-4ce1-843f-029aaab5c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0c2b7a-2b52-47cd-8d78-a5db45c0c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'Albert Einstein'), ('“It is better to be hated for what you are than to be loved for what you are not.”', 'André Gide')]\n"
     ]
    }
   ],
   "source": [
    "all_quotes = soup.select(\"div.quote\")\n",
    "life_quotes = []\n",
    "\n",
    "for q in all_quotes:\n",
    "    all_tags = []\n",
    "\n",
    "    for tag in q.select(\".tags .tag\"):\n",
    "        all_tags.append(tag.get_text())\n",
    "\n",
    "    if \"life\" in all_tags:\n",
    "        text = q.select_one(\"span.text\").get_text()\n",
    "        author = q.select_one(\"small.author\").get_text()\n",
    "        life_quotes.append((text, author))\n",
    "\n",
    "print(life_quotes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
